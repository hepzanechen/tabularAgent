# Preprocessing Module

A comprehensive preprocessing toolkit for TabularML projects, focusing on memory optimization and data preparation utilities.

## üìÅ Module Structure

```
preprocess/
‚îú‚îÄ‚îÄ __init__.py              # Package initialization and exports
‚îú‚îÄ‚îÄ memory_optimizer.py      # Memory optimization utilities
‚îî‚îÄ‚îÄ README.md               # This file
```

## üöÄ Quick Start

```python
# Import the preprocessing utilities
from preprocess import reduce_mem_usage, optimize_dataframe_memory

# Basic memory optimization
df_optimized = reduce_mem_usage(df, "my_dataset")

# Enhanced optimization with categorical conversion
df_enhanced = optimize_dataframe_memory(
    df, 
    dataset_name="my_dataset",
    optimize_strings=True,
    optimize_categories=True
)
```

## üìâ Memory Optimization

### `reduce_mem_usage(dataframe, dataset)`

**Optimizes DataFrame memory by downcasting numeric types.**

**Features:**
- ‚úÖ Downcasts integers: `int64` ‚Üí `int8/int16/int32` as appropriate
- ‚úÖ Downcasts floats: `float64` ‚Üí `float16/float32` with precision checks
- ‚úÖ Preserves data integrity with range validation
- ‚úÖ Detailed memory usage reporting

**Example:**
```python
import pandas as pd
import numpy as np
from preprocess import reduce_mem_usage

# Create sample data
df = pd.DataFrame({
    'small_int': np.random.randint(0, 100, 10000),      # int64 ‚Üí int8
    'large_float': np.random.randn(10000),              # Keep float64
    'medium_int': np.random.randint(0, 10000, 10000)    # int64 ‚Üí int16
})

# Optimize memory
df_optimized = reduce_mem_usage(df, "sample_data")
```

**Output:**
```
Reducing memory usage for: sample_data
--- Memory usage before: 0.23 MB
--- Memory usage after: 0.08 MB
--- Decreased memory usage by 65.2%
```

### `optimize_dataframe_memory(dataframe, **options)`

**Enhanced memory optimization with categorical conversion and advanced features.**

**Parameters:**
- `dataset_name`: Name for logging purposes
- `optimize_strings`: Convert string columns to categories (default: True)
- `optimize_categories`: Optimize existing categorical columns (default: True)
- `inplace`: Modify original DataFrame (default: False)

**Example:**
```python
df_enhanced = optimize_dataframe_memory(
    df,
    dataset_name="Enhanced Dataset",
    optimize_strings=True,
    optimize_categories=True,
    inplace=False
)
```

## üîç Analysis Utilities

### `suggest_memory_optimizations(df)`

**Analyzes DataFrame and suggests optimization strategies.**

```python
from preprocess.memory_optimizer import suggest_memory_optimizations

suggestions = suggest_memory_optimizations(df)
for suggestion in suggestions:
    print(f"üí° {suggestion}")
```

### `get_detailed_memory_info(df)`

**Returns comprehensive memory usage statistics.**

```python
from preprocess.memory_optimizer import get_detailed_memory_info

memory_info = get_detailed_memory_info(df)
print(f"Total memory: {memory_info['total_mb']:.2f} MB")
print(f"Shape: {memory_info['shape']}")

# Memory by data type
for dtype, info in memory_info['by_dtype'].items():
    print(f"{dtype}: {info['total_memory_mb']:.2f} MB")
```

### `compare_memory_usage(df_before, df_after, dataset_name)`

**Compares memory usage between DataFrames.**

```python
from preprocess.memory_optimizer import compare_memory_usage

compare_memory_usage(df_original, df_optimized, "My Dataset")
```

**Output:**
```
üìä Memory Comparison for My Dataset
==================================================
Before optimization: 1.50 MB
After optimization:  0.52 MB
Memory saved:        0.98 MB
Reduction:           65.3%

üìà Dtype Changes:
  large_int: int64 ‚Üí int8 (0.08 MB ‚Üí 0.01 MB)
  medium_int: int64 ‚Üí int16 (0.08 MB ‚Üí 0.02 MB)
```

## üß™ Quick Test

Test the memory optimization utilities:

```python
import pandas as pd
import numpy as np
from preprocess import reduce_mem_usage

# Create test data
df = pd.DataFrame({
    'int_col': np.random.randint(0, 100, 1000),
    'float_col': np.random.randn(1000)
})

# Optimize memory
df_optimized = reduce_mem_usage(df, "test_data")
```

## üí° Best Practices

### For TabPFN Integration

**TabPFN has specific constraints that memory optimization helps with:**

```python
# Optimize before using with TabPFN
df_optimized = optimize_dataframe_memory(df, "tabpfn_data")

# Check TabPFN compatibility
print(f"Samples: {len(df_optimized):,} (TabPFN limit: 10,000)")
print(f"Features: {len(df_optimized.columns)} (TabPFN limit: 100)")
print(f"Memory: {get_memory_usage(df_optimized):.2f} MB")
```

### Memory Optimization Workflow

1. **Analyze first:**
   ```python
   suggestions = suggest_memory_optimizations(df)
   ```

2. **Apply basic optimization:**
   ```python
   df_opt = reduce_mem_usage(df, "my_data")
   ```

3. **Enhanced optimization if needed:**
   ```python
   df_enhanced = optimize_dataframe_memory(df_opt, optimize_strings=True)
   ```

4. **Verify results:**
   ```python
   compare_memory_usage(df, df_enhanced, "Final")
   ```

### Data Type Guidelines

| **Original Type** | **Optimized To** | **Condition** |
|------------------|------------------|---------------|
| `int64` | `int8` | Values in [-128, 127] |
| `int64` | `int16` | Values in [-32,768, 32,767] |
| `int64` | `int32` | Values in [-2B, 2B] |
| `float64` | `float32` | No precision loss |
| `float64` | `float16` | Small values, no precision loss |
| `object` | `category` | <50% unique values |

## ‚ö†Ô∏è Important Notes

### Precision Considerations

- **Float16**: Can lose precision. Only used when values are small and precision loss is acceptable
- **Float32**: Generally safe for most ML applications
- **Categories**: Test downstream compatibility

### Performance Impact

- **Memory**: 50-80% reduction typical
- **Speed**: Categorical columns can be faster for groupby operations
- **GPU**: Smaller dtypes mean more data fits in GPU memory

### Integration with TabularML

```python
# Example integration with your TabularML workflow
from entrance.tabular_ml import TabularML
from preprocess import optimize_dataframe_memory

# Optimize before ML training
df_optimized = optimize_dataframe_memory(df, "training_data")

# Use with TabularML
ml = TabularML(df=df_optimized, target_col='target', problem_type='regression')
ml.train_on_raw_features_with_tuning(engine_name='tabpfn', test_size=0.2)
```

## üîÆ Future Extensions

The preprocessing module is designed to be extensible. Planned additions:

- `data_cleaner.py`: Missing value imputation, outlier detection
- `feature_preprocessor.py`: Scaling, encoding, feature engineering
- `batch_processor.py`: Chunked processing for very large datasets
- `validation.py`: Data quality checks and validation

## üìä Example Results

**Typical memory reductions achieved:**

| **Dataset Type** | **Memory Reduction** | **Speed Improvement** |
|-----------------|---------------------|---------------------|
| Numeric-heavy | 60-80% | 1.5-2x faster |
| Mixed types | 40-60% | 1.2-1.5x faster |
| Categorical-heavy | 70-90% | 2-3x faster |

## ü§ù Contributing

To add new preprocessing utilities:

1. Create new `.py` file in `preprocess/`
2. Add imports to `__init__.py`
3. Update this README
4. Add demonstrations to `demo_usage.py`

---

**Made for TabularML Project** üéØ 